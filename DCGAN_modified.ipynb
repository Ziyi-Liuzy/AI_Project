{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The optimized DCGAN\n",
    "\n",
    "In order to improve the clarity and detail capture ability of the generated image, the DCGAN model is improved, mainly by increasing the input size of the image. In order to accommodate the larger image size, the structure of the generator and discriminator has also been adjusted, mainly by increasing the number of convolutional layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter configuration\n",
    "\n",
    "\n",
    "The code defines a class called Config that contains Settings for some configuration parameters.\n",
    "\n",
    "These parameters include the path to save the results, the model path of the discriminator and generator, the image path and the size of the image, the batch size, the maximum number of training rounds, the noise vector dimension, and the number of feature channels.\n",
    "\n",
    "In addition, the code checks if the results folder and snapshot folder exist, and creates them if they do not.\n",
    "\n",
    "### Expand the input size of the image to 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "\n",
    "    # Path setting\n",
    "    result_save_path = 'results/' \n",
    "    d_net_path = 'snapshots/dnet.pth' \n",
    "    g_net_path = 'snapshots/gnet.pth' \n",
    "    img_path = 'painting/' \n",
    "\n",
    "    # The image size is adjusted to 256\n",
    "    img_size = 256 \n",
    "    batch_size = 256 \n",
    "    max_epoch = 300 \n",
    "    noise_dim = 100 \n",
    "    feats_channel = 64 \n",
    "\n",
    "opt = Config() \n",
    "\n",
    "# Generate the result folder and snapshots folder\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results') \n",
    "if not os.path.exists('snapshots'):\n",
    "    os.mkdir('snapshots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generater design\n",
    "\n",
    "The initial layer of the generated network has significantly increased the number of feature maps generated from noise, and more layers have been introduced, such as additional layers of self.feats * 8 and self.feats * 4, which enhance the learning ability of the model to generate higher quality images, This is especially true when working with larger (256x256) images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gnet(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Gnet, self).__init__()\n",
    "        self.feats = opt.feats_channel\n",
    "        self.generate = nn.Sequential(\n",
    "\n",
    "            # Initial layer: Generate more feature maps from noise\n",
    "            nn.ConvTranspose2d(opt.noise_dim, self.feats * 16, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(self.feats * 16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Gradually increase the size and complexity of the feature map\n",
    "            nn.ConvTranspose2d(self.feats * 16, self.feats * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.feats * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(self.feats * 8, self.feats * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.feats * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(self.feats * 8, self.feats * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.feats * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(self.feats * 4, self.feats * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.feats * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(self.feats * 4, self.feats * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.feats * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Final layer: Generates the target image size (256)\n",
    "            nn.ConvTranspose2d(self.feats * 2, 3, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.generate(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminantor design\n",
    "\n",
    "Correspondingly, the discriminant network has also been deepened by adding convolutional layers and gradually increasing the number of feature maps to self.feats * 16, hoping to improve the quality of generated images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dnet(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Dnet, self).__init__()\n",
    "        self.feats = opt.feats_channel\n",
    "        self.discrim = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=self.feats, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # size: (feats, 128, 128)\n",
    "            \n",
    "            nn.Conv2d(self.feats, self.feats * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.feats * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # size: (feats*2, 64, 64)\n",
    "            \n",
    "            nn.Conv2d(self.feats * 2, self.feats * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.feats * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # size: (feats*4, 32, 32)\n",
    "            \n",
    "            nn.Conv2d(self.feats * 4, self.feats * 8, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.feats * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # size: (feats*8, 16, 16)\n",
    "\n",
    "            nn.Conv2d(self.feats * 8, self.feats * 16, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(self.feats * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # size: (feats*16, 8, 8)\n",
    "\n",
    "            nn.Conv2d(self.feats * 16, 1, kernel_size=8, stride=1, padding=0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # final size: (1, 1, 1)\n",
    "            # Represents the authenticity of the image\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discrim(x).view(-1)\n",
    "\n",
    "\n",
    "g_net, d_net = Gnet(opt), Dnet(opt) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A series of image preprocessing operations are defined and a data loader is created.\n",
    "\n",
    "2. Set up the device (CPU or CUDA), move the generator and discriminator to the device,\n",
    "\n",
    "3. Optimizers, loss functions, and labels are defined.\n",
    "\n",
    "4. The random noise for training is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose([\n",
    "    # resize Image size\n",
    "    torchvision.transforms.Resize(opt.img_size),\n",
    "    # Center crop image size\n",
    "    torchvision.transforms.CenterCrop(opt.img_size), \n",
    "    # array tensor.float(), adapted to the data format of the torch framework\n",
    "    torchvision.transforms.ToTensor() \n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(root=opt.img_path, transform=transforms)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=opt.batch_size,\n",
    "    num_workers = 0,\n",
    "    drop_last = True\n",
    ")\n",
    "\n",
    "# set device(cpu or cuda)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "g_net.to(device)\n",
    "d_net.to(device)\n",
    "\n",
    "optimize_g = torch.optim.Adam(g_net.parameters(), lr= 2e-4, betas=(0.5, 0.999))\n",
    "optimize_d = torch.optim.Adam(d_net.parameters(), lr= 2e-4, betas=(0.5, 0.999))\n",
    "# optimize_d = torch.optim.SGD(d_net.parameters(), lr= 2e-4)\n",
    "\n",
    "#BCEloss, find the binary classification probability\n",
    "criterions = nn.BCELoss().to(device) \n",
    "\n",
    "# Define the tag and start injecting the generator's input noise\n",
    "true_labels = torch.ones(opt.batch_size).to(device) \n",
    "fake_labels = torch.zeros(opt.batch_size).to(device) \n",
    "\n",
    "# Generate N(1,1) standard normal distribution, 100 dimensional, 256 numbers of random noise\n",
    "noises = torch.randn(opt.batch_size, opt.noise_dim, 1, 1).to(device)\n",
    "\n",
    "test_noises = torch.randn(opt.batch_size, opt.noise_dim, 1, 1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-trained generator and discriminator model weight files, and then execute the training process.\n",
    "\n",
    "Within each training cycle, the training is done by looping through the images in the data loader. The discriminator is trained 5 times per session, and the generator is trained 1 time per session.\n",
    "\n",
    "During the training process, the losses of discriminator and generator are calculated and optimized, and the model parameters are updated.\n",
    "\n",
    "At the end of each training cycle, the generative network generates a batch of images and saves a portion of them as a result. At the same time, the weights of the model are saved and the loss values of the discriminator and generator for the current training cycle are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load failed, retrain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [01:02,  8.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 ---D-loss:--- 0.1787092238664627 ---G-loss:--- 5.678250312805176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:26,  8.93s/it]"
     ]
    }
   ],
   "source": [
    "# Load weight file\n",
    "try:\n",
    "    g_net.load_state_dict(torch.load(opt.g_net_path)) \n",
    "    d_net.load_state_dict(torch.load(opt.d_net_path))\n",
    "    print('Load successfully. Continue training')\n",
    "except:\n",
    "    print('Load failed, retrain')\n",
    "\n",
    "for epoch in range(opt.max_epoch): \n",
    "\n",
    "    for itertion, (img, _) in tqdm((enumerate(dataloader))): \n",
    "        real_img = img.to(device)\n",
    "\n",
    "        # The discriminator is trained 5 times and the generator is trained 1 time\n",
    "        if itertion % 1 == 0:\n",
    "\n",
    "            optimize_d.zero_grad() \n",
    "            \n",
    "            # Real data input discriminant network\n",
    "            output = d_net(real_img) \n",
    "            # The discriminator is expected to identify the real image as a positive sample with a label of 1\n",
    "            d_real_loss = criterions(output, true_labels)\n",
    "            fake_image = g_net(noises.detach()).detach() \n",
    "\n",
    "            # Generate data input to the discriminant network\n",
    "            output = d_net(fake_image) \n",
    "            # The discriminator is expected to identify the generated image as a negative sample with a label of 0\n",
    "            d_fake_loss = criterions(output, fake_labels)\n",
    "\n",
    "            # loss Fusion calculation\n",
    "            d_loss = (d_fake_loss + d_real_loss) / 2 \n",
    "\n",
    "            d_loss.backward() \n",
    "            optimize_d.step() \n",
    "\n",
    "        # Generate network optimizer gradient clear\n",
    "        if itertion % 1 == 0:\n",
    "            optimize_g.zero_grad() \n",
    "            noises.data.copy_(torch.randn(opt.batch_size, opt.noise_dim, 1, 1)) \n",
    "\n",
    "            # Calculate the probability that the generated image is real\n",
    "            fake_image = g_net(noises) \n",
    "            output = d_net(fake_image) \n",
    "            g_loss = criterions(output, true_labels) \n",
    "\n",
    "            g_loss.backward() \n",
    "            optimize_g.step() \n",
    "\n",
    "    # randomly generate 256 noises\n",
    "    vid_fake_image = g_net(test_noises) \n",
    "    # Save the first 16 images\n",
    "    torchvision.utils.save_image(vid_fake_image.data[:16], \"%s/%s.png\" % (opt.result_save_path, epoch), normalize=True) \n",
    "    torch.save(d_net.state_dict(),  opt.d_net_path) \n",
    "    torch.save(g_net.state_dict(),  opt.g_net_path) \n",
    "    # loss visualization\n",
    "    print('epoch:', epoch, '---D-loss:---', d_loss.item(), '---G-loss:---', g_loss.item()) \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
