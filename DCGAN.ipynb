{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN\n",
    "\n",
    "## code reference\n",
    "\n",
    "The overall structure of the DCGAN model refers to the code in the following link, with many changes made to the number of deconvolution layers and parameter settings when used.\n",
    "\n",
    "https://github.com/HuiiJi/GAN?tab=readme-ov-file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter configuration\n",
    "\n",
    "The code defines a class called Config that contains Settings for some configuration parameters.\n",
    "\n",
    "These parameters include the path to save the results, the model path of the discriminator and generator, the image path and the size of the image, the batch size, the maximum number of training rounds, the noise vector dimension, and the number of feature channels.\n",
    "\n",
    "In addition, the code checks if the results folder and snapshot folder exist, and creates them if they do not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "\n",
    "    # Path setting\n",
    "    result_save_path = 'results/' \n",
    "    d_net_path = 'DCGAN_snapshots/dnet.pth' \n",
    "    g_net_path = 'DCGAN_snapshots/gnet.pth' \n",
    "    img_path = 'painting/' \n",
    "\n",
    "    img_size = 96 \n",
    "    batch_size = 256 \n",
    "    max_epoch = 300 \n",
    "    noise_dim = 100 \n",
    "    feats_channel = 64 \n",
    "\n",
    "opt = Config() \n",
    "\n",
    "# Generate the result folder and snapshots folder\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')  \n",
    "if not os.path.exists('DCGAN_snapshots'):\n",
    "    os.mkdir('DCGAN_snapshots') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generater design\n",
    "\n",
    "A generator is defined whose constructor __init__() contains a series of layers for the generator network structure, including a transposed convolution layer, a batch normalization layer, and an activation function layer.\n",
    "\n",
    "The generator receives the input noise vector, goes through a series of transposition convolution operations, and finally outputs a composite image similar to the target image.\n",
    "\n",
    "The forward() function is used to perform the forward propagation operation of the generator, returning the generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gnet(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Gnet, self).__init__()\n",
    "        self.feats = opt.feats_channel\n",
    "        self.generate = nn.Sequential(\n",
    "\n",
    "           #input = (n, c, h, w = 256, 100, 1, 1)\n",
    "            nn.ConvTranspose2d(in_channels=opt.noise_dim, out_channels=self.feats * 8, kernel_size=4, stride=1, padding=0,\n",
    "                               bias=False),\n",
    "            nn.BatchNorm2d(self.feats * 8),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # deconv = (input - 1 ) * stride + k -2* padding = (1- 1)*1  +4-0 = 4\n",
    "            #output = (256, 800 ,4, 4)\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=self.feats * 8, out_channels=self.feats * 4, kernel_size=4, stride=2, padding=1,\n",
    "                               bias=False),\n",
    "            nn.BatchNorm2d(self.feats * 4),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # decon = (input - 1)*stride + k - 2*padding = (4-1)*2 + 4-2 = 8\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=self.feats * 4, out_channels=self.feats * 2, kernel_size=4, stride=2, padding=1,\n",
    "                               bias=False),\n",
    "            nn.BatchNorm2d(self.feats * 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # decon = (input - 1)*stride + k - 2*padding = (8-1)*2 + 4-2 = 16\n",
    "\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=self.feats * 2, out_channels=self.feats, kernel_size=4, stride=2, padding=1,\n",
    "                               bias=False),\n",
    "            nn.BatchNorm2d(self.feats),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            # decon = (input - 1)*stride + k - 2*padding = (16-1)*2 + 4-2 = 32\n",
    "\n",
    "            nn.ConvTranspose2d(in_channels=self.feats, out_channels=3, kernel_size=5, stride=3, padding=1, bias=False),\n",
    "\n",
    "            nn.Tanh(),\n",
    "            # decon = (input - 1)*stride + k - 2*padding = (32-1)*3 + 5-2 = 96\n",
    "            #output = (n, c, h, w = 256, 3, 96, 96)\n",
    "\n",
    "    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.generate(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminantor design\n",
    "\n",
    "A simple convolution discriminator is defined.\n",
    "\n",
    "The discriminator takes an image as input, passes through a series of convolution and activation function layers, and finally outputs a scalar value between 0 and 1 that represents the probability that the input image is a real image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dnet(nn.Module):\n",
    "    def __init__(self, opt):\n",
    "        super(Dnet, self).__init__()\n",
    "        self.feats = opt.feats_channel\n",
    "        self.discrim = nn.Sequential(\n",
    "\n",
    "            #input = ï¼ˆn, c, h, w = 256, 3, 96, 96)\n",
    "\n",
    "            nn.Conv2d(in_channels=3, out_channels= self.feats, kernel_size= 5, stride= 3, padding= 1, bias=False),\n",
    "            nn.LeakyReLU(negative_slope=0.2, inplace= True),\n",
    "            #con = (input - k + 2 * padding ) / stride +  1 = (256 - 5 + 2) / 3 + 1 = 128\n",
    "\n",
    "\n",
    "\n",
    "            nn.Conv2d(in_channels= self.feats, out_channels= self.feats * 2, kernel_size= 4, stride= 2, padding= 1, bias=False),\n",
    "            nn.BatchNorm2d(self.feats* 2),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "            nn.Conv2d(in_channels= self.feats * 2, out_channels= self.feats * 4, kernel_size= 4, stride= 2, padding= 1,bias=False),\n",
    "            nn.BatchNorm2d(self.feats * 4),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "\n",
    "            nn.Conv2d(in_channels= self.feats * 4, out_channels= self.feats * 8, kernel_size= 4, stride= 2, padding= 1, bias=False),\n",
    "            nn.BatchNorm2d(self.feats *8),\n",
    "            nn.LeakyReLU(0.2, True),\n",
    "\n",
    "\n",
    "            nn.Conv2d(in_channels= self.feats * 8, out_channels= 1, kernel_size= 4, stride= 1, padding= 0, bias=True),\n",
    "\n",
    "            nn.Sigmoid()\n",
    "\n",
    "            #output = ( n, c, h, w = 256, 1, 1, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.discrim(x).view(-1) \n",
    "\n",
    "g_net, d_net = Gnet(opt), Dnet(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. A series of image preprocessing operations are defined and a data loader is created.\n",
    "\n",
    "2. Set up the device (CPU or CUDA), move the generator and discriminator to the device,\n",
    "\n",
    "3. Optimizers, loss functions, and labels are defined.\n",
    "\n",
    "4. The random noise for training is generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose([\n",
    "    # resize Image size\n",
    "    torchvision.transforms.Resize(opt.img_size), \n",
    "    # Center crop image size\n",
    "    torchvision.transforms.CenterCrop(opt.img_size),\n",
    "    # array tensor.float(), adapted to the data format of the torch framework\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(root=opt.img_path, transform=transforms)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=opt.batch_size, \n",
    "    num_workers = 0,\n",
    "    drop_last = True\n",
    ")\n",
    "\n",
    "# set device(cpu or cuda)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "g_net.to(device)\n",
    "d_net.to(device)\n",
    "\n",
    "optimize_g = torch.optim.Adam(g_net.parameters(), lr= 2e-4, betas=(0.5, 0.999))\n",
    "optimize_d = torch.optim.Adam(d_net.parameters(), lr= 2e-4, betas=(0.5, 0.999))\n",
    "# optimize_d = torch.optim.SGD(d_net.parameters(), lr= 2e-4)\n",
    "\n",
    "#BCEloss, find the binary classification probability\n",
    "criterions = nn.BCELoss().to(device) \n",
    "\n",
    "# Define the tag and start injecting the generator's input noise\n",
    "true_labels = torch.ones(opt.batch_size).to(device) \n",
    "fake_labels = torch.zeros(opt.batch_size).to(device) \n",
    "\n",
    "# Generate N(1,1) standard normal distribution, 100 dimensional, 256 numbers of random noise\n",
    "noises = torch.randn(opt.batch_size, opt.noise_dim, 1, 1).to(device)\n",
    "\n",
    "test_noises = torch.randn(opt.batch_size, opt.noise_dim, 1, 1).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-trained generator and discriminator model weight files, and then execute the training process.\n",
    "\n",
    "Within each training cycle, the training is done by looping through the images in the data loader. The discriminator is trained 5 times per session, and the generator is trained 1 time per session.\n",
    "\n",
    "During the training process, the losses of discriminator and generator are calculated and optimized, and the model parameters are updated.\n",
    "\n",
    "At the end of each training cycle, the generative network generates a batch of images and saves a portion of them as a result. At the same time, the weights of the model are saved and the loss values of the discriminator and generator for the current training cycle are printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load weight file\n",
    "try:\n",
    "    g_net.load_state_dict(torch.load(opt.g_net_path)) \n",
    "    d_net.load_state_dict(torch.load(opt.d_net_path))\n",
    "    print('Load successfully. Continue training')\n",
    "except:\n",
    "    print('Load failed, retrain')\n",
    "\n",
    "for epoch in range(opt.max_epoch):  \n",
    "\n",
    "    for itertion, (img, _) in tqdm((enumerate(dataloader))):\n",
    "        real_img = img.to(device)\n",
    "\n",
    "        # The discriminator is trained 5 times and the generator is trained 1 time\n",
    "        if itertion % 1 == 0: \n",
    "\n",
    "            optimize_d.zero_grad() \n",
    "\n",
    "            # Real data input discriminant network\n",
    "            output = d_net(real_img)\n",
    "            # The discriminator is expected to identify the real image as a positive sample with a label of 1\n",
    "            d_real_loss = criterions(output, true_labels) \n",
    "            fake_image = g_net(noises.detach()).detach() \n",
    "\n",
    "            # Generate data input to the discriminant network\n",
    "            output = d_net(fake_image) \n",
    "            # The discriminator is expected to identify the generated image as a negative sample with a label of 0\n",
    "            d_fake_loss = criterions(output, fake_labels)\n",
    "\n",
    "            #loss Fusion calculation\n",
    "            d_loss = (d_fake_loss + d_real_loss) / 2 \n",
    "            \n",
    "\n",
    "            d_loss.backward() \n",
    "            optimize_d.step() \n",
    "\n",
    "        # Generate network optimizer gradient clear\n",
    "        if itertion % 1 == 0:\n",
    "            optimize_g.zero_grad() \n",
    "            noises.data.copy_(torch.randn(opt.batch_size, opt.noise_dim, 1, 1)) \n",
    "\n",
    "            # Calculate the probability that the generated image is real\n",
    "            fake_image = g_net(noises) \n",
    "            output = d_net(fake_image) \n",
    "            g_loss = criterions(output, true_labels)\n",
    "\n",
    "            g_loss.backward() \n",
    "            optimize_g.step() \n",
    "\n",
    "    # randomly generate 256 noises\n",
    "    vid_fake_image = g_net(test_noises) \n",
    "    # Save the first 16 images\n",
    "    torchvision.utils.save_image(vid_fake_image.data[:16], \"%s/%s.png\" % (opt.result_save_path, epoch), normalize=True) \n",
    "    torch.save(d_net.state_dict(),  opt.d_net_path)\n",
    "    torch.save(g_net.state_dict(),  opt.g_net_path)\n",
    "    #loss visualization\n",
    "    print('epoch:', epoch, '---D-loss:---', d_loss.item(), '---G-loss:---', g_loss.item()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
